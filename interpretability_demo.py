import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
from prototype_bottleneck import PrototypeBottleneckModel
from mnist_example import load_mnist_data, evaluate_model
import warnings
warnings.filterwarnings('ignore')


class InterpretabilityAnalyzer:
    """
    Comprehensive interpretability analysis for the Prototype Bottleneck Model.
    """
    
    def __init__(self, model, device='cpu'):
        self.model = model
        self.device = device
        self.model.eval()
    
    def analyze_prototype_behavior(self, data_loader, num_samples=1000):
        """
        Analyze how prototypes behave across different classes.
        
        Args:
            data_loader: Data loader for analysis
            num_samples: Number of samples to analyze
            
        Returns:
            analysis_results: Dictionary with analysis results
        """
        print("üîç Analyzing prototype behavior...")
        
        all_prototype_weights = []
        all_labels = []
        all_predictions = []
        sample_count = 0
        
        with torch.no_grad():
            for data, target in data_loader:
                if sample_count >= num_samples:
                    break
                    
                data, target = data.to(self.device), target.to(self.device)
                data = data.view(data.size(0), -1)
                
                predictions, prototype_weights = self.model.predict_with_explanations(data)
                
                all_prototype_weights.append(prototype_weights.cpu().numpy())
                all_labels.extend(target.cpu().numpy())
                all_predictions.extend(predictions.cpu().numpy())
                
                sample_count += data.size(0)
        
        all_prototype_weights = np.concatenate(all_prototype_weights, axis=0)
        all_labels = np.array(all_labels)
        all_predictions = np.array(all_predictions)
        
        # Calculate prototype-class correlations
        prototype_class_corr = np.zeros((10, all_prototype_weights.shape[1]))
        for class_idx in range(10):
            class_mask = all_labels == class_idx
            if class_mask.sum() > 0:
                prototype_class_corr[class_idx] = all_prototype_weights[class_mask].mean(axis=0)
        
        # Calculate prototype specialization
        prototype_specialization = np.max(prototype_class_corr, axis=0)
        most_specialized_prototypes = np.argsort(prototype_specialization)[::-1]
        
        # Calculate prediction confidence vs prototype usage
        confidences = []
        for i in range(len(all_predictions)):
            data_sample = torch.tensor(all_prototype_weights[i:i+1], device=self.device)
            logits, _, _, _ = self.model.forward(data_sample)
            confidence = F.softmax(logits, dim=1).max().item()
            confidences.append(confidence)
        
        confidences = np.array(confidences)
        
        analysis_results = {
            'prototype_weights': all_prototype_weights,
            'labels': all_labels,
            'predictions': all_predictions,
            'prototype_class_corr': prototype_class_corr,
            'prototype_specialization': prototype_specialization,
            'most_specialized_prototypes': most_specialized_prototypes,
            'confidences': confidences
        }
        
        return analysis_results
    
    def visualize_prototype_specialization(self, analysis_results):
        """
        Visualize which prototypes are most specialized for specific classes.
        
        Args:
            analysis_results: Results from analyze_prototype_behavior
        """
        prototype_class_corr = analysis_results['prototype_class_corr']
        most_specialized = analysis_results['most_specialized_prototypes']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Heatmap of prototype-class correlations
        sns.heatmap(prototype_class_corr, 
                   annot=True, fmt='.3f', cmap='Blues',
                   xticklabels=[f'P{i}' for i in range(prototype_class_corr.shape[1])],
                   yticklabels=[f'Class {i}' for i in range(10)],
                   ax=axes[0, 0])
        axes[0, 0].set_title('Prototype-Class Correlation Matrix')
        
        # Prototype specialization scores
        axes[0, 1].bar(range(len(most_specialized)), 
                      analysis_results['prototype_specialization'][most_specialized])
        axes[0, 1].set_title('Prototype Specialization Scores')
        axes[0, 1].set_xlabel('Prototype (sorted by specialization)')
        axes[0, 1].set_ylabel('Specialization Score')
        axes[0, 1].set_xticks(range(len(most_specialized)))
        axes[0, 1].set_xticklabels([f'P{p}' for p in most_specialized])
        
        # Top prototypes for each class
        top_prototypes_per_class = np.argmax(prototype_class_corr, axis=1)
        axes[1, 0].bar(range(10), top_prototypes_per_class)
        axes[1, 0].set_title('Most Important Prototype per Class')
        axes[1, 0].set_xlabel('Class')
        axes[1, 0].set_ylabel('Prototype Index')
        axes[1, 0].set_xticks(range(10))
        
        # Confidence vs prototype usage
        prototype_usage = analysis_results['prototype_weights'].mean(axis=0)
        axes[1, 1].scatter(prototype_usage, analysis_results['confidences'], alpha=0.6)
        axes[1, 1].set_title('Confidence vs Prototype Usage')
        axes[1, 1].set_xlabel('Average Prototype Usage')
        axes[1, 1].set_ylabel('Prediction Confidence')
        
        plt.tight_layout()
        plt.show()
    
    def create_interactive_explanation_dashboard(self, analysis_results):
        """
        Create an interactive dashboard for exploring model explanations.
        
        Args:
            analysis_results: Results from analyze_prototype_behavior
        """
        print("üìä Creating interactive explanation dashboard...")
        
        # Create subplots
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('Prototype Usage Distribution', 'Class-Prototype Heatmap',
                          'Confidence vs Prototype Usage', 'Prototype Specialization'),
            specs=[[{"type": "bar"}, {"type": "heatmap"}],
                   [{"type": "scatter"}, {"type": "bar"}]]
        )
        
        # 1. Prototype usage distribution
        prototype_usage = analysis_results['prototype_weights'].mean(axis=0)
        fig.add_trace(
            go.Bar(x=[f'P{i}' for i in range(len(prototype_usage))], 
                  y=prototype_usage, name='Average Usage'),
            row=1, col=1
        )
        
        # 2. Class-prototype heatmap
        fig.add_trace(
            go.Heatmap(z=analysis_results['prototype_class_corr'],
                      x=[f'P{i}' for i in range(analysis_results['prototype_class_corr'].shape[1])],
                      y=[f'Class {i}' for i in range(10)],
                      colorscale='Blues'),
            row=1, col=2
        )
        
        # 3. Confidence vs prototype usage
        fig.add_trace(
            go.Scatter(x=prototype_usage, y=analysis_results['confidences'],
                      mode='markers', name='Samples',
                      marker=dict(size=8, opacity=0.6)),
            row=2, col=1
        )
        
        # 4. Prototype specialization
        most_specialized = analysis_results['most_specialized_prototypes']
        specialization_scores = analysis_results['prototype_specialization'][most_specialized]
        fig.add_trace(
            go.Bar(x=[f'P{p}' for p in most_specialized], 
                  y=specialization_scores, name='Specialization'),
            row=2, col=2
        )
        
        fig.update_layout(height=800, title_text="Prototype Bottleneck Model Interpretability Dashboard")
        fig.show()
    
    def explain_individual_predictions(self, data_loader, num_examples=5):
        """
        Provide detailed explanations for individual predictions.
        
        Args:
            data_loader: Data loader
            num_examples: Number of examples to explain
        """
        print(f"üí° Explaining {num_examples} individual predictions...")
        
        data_iter = iter(data_loader)
        images, labels = next(data_iter)
        images, labels = images.to(self.device), labels.to(self.device)
        
        fig, axes = plt.subplots(num_examples, 4, figsize=(20, 5*num_examples))
        
        for i in range(num_examples):
            # Get explanation
            explanation = self.model.explain_prediction(images[i:i+1].view(1, -1))
            
            # Original image
            axes[i, 0].imshow(images[i].squeeze().cpu(), cmap='gray')
            axes[i, 0].set_title(f'Input\nTrue: {labels[i]}, Pred: {explanation["prediction"]}')
            axes[i, 0].axis('off')
            
            # Prototype weights
            weights = explanation['prototype_weights']
            axes[i, 1].bar(range(len(weights)), weights)
            axes[i, 1].set_title(f'Prototype Weights\nConfidence: {explanation["confidence"]:.3f}')
            axes[i, 1].set_xlabel('Prototype')
            axes[i, 1].set_ylabel('Weight')
            
            # Top 3 prototypes
            top_3 = explanation['top_prototypes'][:3]
            top_3_weights = weights[top_3]
            axes[i, 2].bar(range(3), top_3_weights)
            axes[i, 2].set_title('Top 3 Prototypes')
            axes[i, 2].set_xlabel('Prototype')
            axes[i, 2].set_ylabel('Weight')
            axes[i, 2].set_xticks(range(3))
            axes[i, 2].set_xticklabels([f'P{p}' for p in top_3])
            
            # Reconstruction error
            axes[i, 3].text(0.5, 0.5, f'Reconstruction Error:\n{explanation["reconstruction_error"]:.4f}',
                           ha='center', va='center', transform=axes[i, 3].transAxes,
                           fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
            axes[i, 3].set_title('Model Confidence')
            axes[i, 3].axis('off')
        
        plt.tight_layout()
        plt.show()
    
    def analyze_prototype_evolution(self, model_checkpoints, data_loader):
        """
        Analyze how prototypes evolve during training.
        
        Args:
            model_checkpoints: List of model states at different epochs
            data_loader: Data loader for evaluation
        """
        print("üîÑ Analyzing prototype evolution during training...")
        
        evolution_data = []
        
        for epoch, checkpoint in enumerate(model_checkpoints):
            # Load checkpoint
            self.model.load_state_dict(checkpoint)
            self.model.eval()
            
            # Evaluate on a subset
            accuracy, _, _, prototype_weights = evaluate_model(self.model, data_loader, self.device)
            
            # Calculate prototype diversity
            prototype_diversity = np.std(prototype_weights.mean(axis=0))
            
            evolution_data.append({
                'epoch': epoch,
                'accuracy': accuracy,
                'prototype_diversity': prototype_diversity,
                'prototype_usage': prototype_weights.mean(axis=0)
            })
        
        # Plot evolution
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        epochs = [d['epoch'] for d in evolution_data]
        accuracies = [d['accuracy'] for d in evolution_data]
        diversities = [d['prototype_diversity'] for d in evolution_data]
        
        axes[0].plot(epochs, accuracies)
        axes[0].set_title('Accuracy Evolution')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Accuracy (%)')
        axes[0].grid(True, alpha=0.3)
        
        axes[1].plot(epochs, diversities)
        axes[1].set_title('Prototype Diversity Evolution')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Diversity Score')
        axes[1].grid(True, alpha=0.3)
        
        # Prototype usage evolution heatmap
        usage_matrix = np.array([d['prototype_usage'] for d in evolution_data])
        sns.heatmap(usage_matrix.T, ax=axes[2], cmap='Blues',
                   xticklabels=epochs[::5], yticklabels=[f'P{i}' for i in range(usage_matrix.shape[1])])
        axes[2].set_title('Prototype Usage Evolution')
        axes[2].set_xlabel('Epoch')
        axes[2].set_ylabel('Prototype')
        
        plt.tight_layout()
        plt.show()
    
    def generate_explanation_report(self, analysis_results, output_file='interpretability_report.html'):
        """
        Generate a comprehensive HTML report of interpretability analysis.
        
        Args:
            analysis_results: Results from analyze_prototype_behavior
            output_file: Output HTML file path
        """
        print(f"üìÑ Generating interpretability report: {output_file}")
        
        # Calculate summary statistics
        accuracy = np.mean(analysis_results['predictions'] == analysis_results['labels'])
        avg_confidence = np.mean(analysis_results['confidences'])
        avg_reconstruction_error = np.mean([0.01])  # Placeholder
        
        # Create HTML report
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Prototype Bottleneck Model Interpretability Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 10px; }}
                .section {{ margin: 30px 0; padding: 20px; border: 1px solid #ddd; border-radius: 10px; }}
                .metric {{ display: inline-block; margin: 10px; padding: 10px; background-color: #e8f4f8; border-radius: 5px; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ü§ñ Prototype Bottleneck Model Interpretability Report</h1>
                <p>Comprehensive analysis of model transparency and decision-making process</p>
            </div>
            
            <div class="section">
                <h2>üìä Model Performance Summary</h2>
                <div class="metric">
                    <strong>Accuracy:</strong> {accuracy:.2%}
                </div>
                <div class="metric">
                    <strong>Average Confidence:</strong> {avg_confidence:.3f}
                </div>
                <div class="metric">
                    <strong>Prototypes:</strong> {analysis_results['prototype_weights'].shape[1]}
                </div>
            </div>
            
            <div class="section">
                <h2>üéØ Prototype Analysis</h2>
                <h3>Most Specialized Prototypes:</h3>
                <table>
                    <tr><th>Rank</th><th>Prototype</th><th>Specialization Score</th></tr>
        """
        
        most_specialized = analysis_results['most_specialized_prototypes']
        specialization_scores = analysis_results['prototype_specialization'][most_specialized]
        
        for i, (prototype, score) in enumerate(zip(most_specialized, specialization_scores)):
            html_content += f"""
                    <tr><td>{i+1}</td><td>P{prototype}</td><td>{score:.3f}</td></tr>
            """
        
        html_content += """
                </table>
            </div>
            
            <div class="section">
                <h2>üîç Key Insights</h2>
                <ul>
                    <li><strong>Interpretability:</strong> Each prediction can be explained by showing which prototypes were most important</li>
                    <li><strong>Transparency:</strong> Prototype weights provide clear visibility into decision-making process</li>
                    <li><strong>Concept Learning:</strong> Prototypes represent meaningful concepts in the data</li>
                    <li><strong>Confidence:</strong> Model confidence correlates with prototype usage patterns</li>
                </ul>
            </div>
            
            <div class="section">
                <h2>üìà Recommendations</h2>
                <ul>
                    <li>Monitor prototype specialization to ensure diverse concept coverage</li>
                    <li>Use prototype weights for model debugging and error analysis</li>
                    <li>Consider prototype diversity when designing model architecture</li>
                    <li>Leverage explanations for user trust and model validation</li>
                </ul>
            </div>
        </body>
        </html>
        """
        
        with open(output_file, 'w') as f:
            f.write(html_content)
        
        print(f"‚úÖ Report saved to {output_file}")


def main():
    """
    Main function to run the interpretability demo.
    """
    print("üîç Prototype Bottleneck Model - Interpretability Demo")
    print("=" * 60)
    
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Load data
    print("\nüìä Loading MNIST dataset...")
    train_loader, val_loader, test_loader = load_mnist_data(batch_size=64)
    
    # Initialize model (you can load a pre-trained model here)
    print("\nü§ñ Initializing model...")
    model = PrototypeBottleneckModel(
        input_dim=784,
        bottleneck_dim=128,
        num_prototypes=10,
        prototype_dim=784,
        num_classes=10,
        learning_rate=0.001
    ).to(device)
    
    # For demo purposes, we'll train a quick model
    print("\nüéØ Training model for interpretability analysis...")
    history = model.train_model(train_loader, epochs=20, verbose=True)
    
    # Initialize interpretability analyzer
    analyzer = InterpretabilityAnalyzer(model, device)
    
    # Run comprehensive interpretability analysis
    print("\nüîç Running comprehensive interpretability analysis...")
    analysis_results = analyzer.analyze_prototype_behavior(test_loader, num_samples=1000)
    
    # Visualize prototype specialization
    print("\nüìä Visualizing prototype specialization...")
    analyzer.visualize_prototype_specialization(analysis_results)
    
    # Create interactive dashboard
    print("\nüìà Creating interactive explanation dashboard...")
    analyzer.create_interactive_explanation_dashboard(analysis_results)
    
    # Explain individual predictions
    print("\nüí° Explaining individual predictions...")
    analyzer.explain_individual_predictions(test_loader, num_examples=5)
    
    # Generate comprehensive report
    print("\nüìÑ Generating interpretability report...")
    analyzer.generate_explanation_report(analysis_results)
    
    print("\n‚úÖ Interpretability Demo Complete!")
    print("\nüéØ Key Features Demonstrated:")
    print("- Prototype behavior analysis across different classes")
    print("- Individual prediction explanations with prototype weights")
    print("- Interactive visualization dashboard")
    print("- Comprehensive interpretability report")
    print("- Prototype specialization and diversity analysis")


if __name__ == "__main__":
    main()
